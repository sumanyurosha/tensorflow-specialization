{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation with a RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMh9cfj8Pzx5xEIruCVaLM1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumanyurosha/tensorflow-specialization/blob/master/Course3/Week4/Text_Generation_with_a_character_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoS7V2Dd5zIy"
      },
      "source": [
        "**Import Tensorflow and other Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eoPBaRG5hxe"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEaAm7eR57xR"
      },
      "source": [
        "**Download the Shakespeare dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqrLgy8H5sD7",
        "outputId": "02135e73-8d87-401c-a1cc-420eca134345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "path_to_file = keras.utils.get_file(\"shakespeare.txt\", 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydk4d2dr6zCJ"
      },
      "source": [
        "**Read the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsvm_qCi6KBL",
        "outputId": "811ee78e-1572-479e-f49b-65cfd29c10fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
        "\n",
        "print(\"There are {} characters in this dataset\".format(len(text)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1115394 characters in this dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu31DjAF7Mod",
        "outputId": "6f65b9e1-ad39-4660-e9c8-fd3da5f856a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# taking a look at the first 250 characters in the text\n",
        "print(text[:250])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sGSnSPi7XuH",
        "outputId": "62c2a7cb-40ba-4a89-da62-b2d347bba4bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# finding the unique characters in it\n",
        "vocab = sorted(set(text))\n",
        "print(\"There are {} unique characters\".format(len(vocab)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFa7KarETtM8"
      },
      "source": [
        "**Vectorize the Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nfaw8xSn7yYy"
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAajDV7FULkP",
        "outputId": "f6c5cee3-19d6-42c9-e1e9-9711a9bee930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "print(\"{\")\n",
        "for char, _ in zip(char2idx, range(20)):\n",
        "    print(\"{:4s} : {:3d}\".format(repr(char), char2idx[char]))\n",
        "print(\"}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "'\\n' :   0\n",
            "' '  :   1\n",
            "'!'  :   2\n",
            "'$'  :   3\n",
            "'&'  :   4\n",
            "\"'\"  :   5\n",
            "','  :   6\n",
            "'-'  :   7\n",
            "'.'  :   8\n",
            "'3'  :   9\n",
            "':'  :  10\n",
            "';'  :  11\n",
            "'?'  :  12\n",
            "'A'  :  13\n",
            "'B'  :  14\n",
            "'C'  :  15\n",
            "'D'  :  16\n",
            "'E'  :  17\n",
            "'F'  :  18\n",
            "'G'  :  19\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7CKTxn6VJKn",
        "outputId": "f2d1d025-2e8d-4743-8a51-1c33b4eafbdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# taking a look at the first 13 characters of the text mapped to integers\n",
        "print(\" {} characters mapped to integers > {}\".format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 'First Citizen' characters mapped to integers > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJIAwNaKX1rs"
      },
      "source": [
        "**Creating training examples and targets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzvXB_gSWUj1",
        "outputId": "079275d5-c60d-41f9-d5a5-3cb21183b1a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "seq_length = 100\n",
        "\n",
        "examples_per_epochs = len(text) // (seq_length + 1)\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for c in char_dataset.take(5):\n",
        "    print(idx2char[c])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KELUP9CHYc1q",
        "outputId": "dc2bb6b1-ba66-412c-c9a0-942dd80e6745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(\"\".join(idx2char[item.numpy()])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA6M-fp8Y8oM"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKMBl45HZ2Ed",
        "outputId": "98b412f6-2bbb-40a2-b9b4-ee0b6630e5a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input data : {}\".format(repr(\"\".join(idx2char[input_example.numpy()]))))\n",
        "    print(\"Output data: {}\".format(repr(\"\".join(idx2char[target_example.numpy()]))))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data : 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Output data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnmb9pQGaRCa",
        "outputId": "9906817d-bdf1-4da6-95af-0632080e0680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input, target) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step{:4d}\".format(i+1))\n",
        "    print(\"Input : {}({})\".format(input, repr(idx2char[input.numpy()])))\n",
        "    print(\"Expected output: {}({})\".format(target, repr(idx2char[target.numpy()])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step   1\n",
            "Input : 18('F')\n",
            "Expected output: 47('i')\n",
            "Step   2\n",
            "Input : 47('i')\n",
            "Expected output: 56('r')\n",
            "Step   3\n",
            "Input : 56('r')\n",
            "Expected output: 57('s')\n",
            "Step   4\n",
            "Input : 57('s')\n",
            "Expected output: 58('t')\n",
            "Step   5\n",
            "Input : 58('t')\n",
            "Expected output: 1(' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNyzYTr6cmdm"
      },
      "source": [
        "**Creating training batches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXIOvsOnb30_",
        "outputId": "f12abe22-ec0d-4dc6-d4f0-0f1b45065b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnNwSBHfeKWP"
      },
      "source": [
        "**Build the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxwKQ-kMc5hN"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnvdpzlSeUOl"
      },
      "source": [
        "def build_model(vocab_size, batch_size, rnn_units, embedding_dim):\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                               batch_input_shape=[batch_size, None]),\n",
        "        keras.layers.GRU(rnn_units, return_sequences=True,\n",
        "                         stateful=True,\n",
        "                         recurrent_initializer=\"glorot_uniform\"),\n",
        "        keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdo0bUcyfVuq",
        "outputId": "b2cf8e08-06a4-4a7f-bd4d-2e0c2a9c0c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "model = build_model(vocab_size=vocab_size,\n",
        "                    embedding_dim=embedding_dim,\n",
        "                    rnn_units=rnn_units,\n",
        "                    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apbDJPAWjb_4"
      },
      "source": [
        "**Try the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lflfkOCMg8ne",
        "outputId": "164cfc8d-7872-452c-eb46-805239f75434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    predicted_example = model(input_example)\n",
        "\n",
        "# should be batch_size, seq_length, vocab_size\n",
        "print(predicted_example.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1edxxUSvjoik",
        "outputId": "8978dad6-0b37-4ed2-e39f-17880f99ffd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6POn8ixj2c7"
      },
      "source": [
        "sampled_indices = tf.random.categorical(predicted_example[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXU_a2xGkeJj",
        "outputId": "71e24516-6f02-4918-d88f-3b880d1c0539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([42, 58, 57,  7, 39,  1, 54, 39,  0, 41, 22, 44, 44, 39, 48, 28, 28,\n",
              "       24, 46, 14,  6, 12,  9, 35, 62, 56, 63, 29, 24,  9, 27, 28,  9, 43,\n",
              "       64, 18,  2, 33, 10, 52, 44, 45,  4, 13, 19,  0, 15, 50,  6, 42, 61,\n",
              "       31, 11, 16,  4, 50,  1, 50, 13, 22, 55, 58,  2,  1, 23,  1, 15, 46,\n",
              "       33, 59, 12, 35, 28, 47, 37, 21, 30, 20, 49,  5, 63, 63, 44,  7, 37,\n",
              "       26, 22, 32, 27, 60, 47, 34, 18, 52, 29, 10, 42, 11, 29, 41])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4RbFfcfkgCl",
        "outputId": "80115a0f-ac67-44c1-fae0-cae50f4f4587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Input text was : \\n {}\".format(repr(\"\".join(idx2char[input_example[0].numpy()]))))\n",
        "print(\"\")\n",
        "print(\"Predicted output was : \\n{}\".format(repr(\"\".join(idx2char[sampled_indices]))))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text was : \n",
            " '\\nPOMPEY:\\nVery well: you being then, if you be remembered,\\ncracking the stones of the foresaid prunes'\n",
            "\n",
            "Predicted output was : \n",
            "\"dts-a pa\\ncJffajPPLhB,?3WxryQL3OP3ezF!U:nfg&AG\\nCl,dwS;D&l lAJqt! K ChUu?WPiYIRHk'yyf-YNJTOviVFnQ:d;Qc\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-eDhTl3lexP"
      },
      "source": [
        "**Attaching an Optimizer and Loss function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJo2_ONflAnA",
        "outputId": "3292e7da-d599-4ab4-d31a-4ed3b67838e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_loss = loss(target_example, predicted_example)\n",
        "print(\"Scalar loss :\", example_loss.numpy().mean())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scalar loss : 4.1751647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDNsrrpwmOvt"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db4pEHX7mnMG"
      },
      "source": [
        "**Configure checkpoints**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO__iUVOmWiC"
      },
      "source": [
        "checkpoint_dir = \"/checkpoints\"\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX6Hkxb8nI0e"
      },
      "source": [
        "**Executing the Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNkIiOO4nHLE",
        "outputId": "23cb4ee2-1ac5-4758-bb33-dc5b29a30e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 30\n",
        "\n",
        "model.fit(dataset, epochs=epochs, callbacks=[checkpoint_callback], verbose=1)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 2.6470\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.9620\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.6910\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.5422\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.4538\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.3955\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.3492\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.3117\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.2760\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.2440\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.2119\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 1.1801\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.1478\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.1144\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.0787\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.0434\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 1.0095\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.9720\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.9384\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.9032\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.8723\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.8435\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.8162\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.7919\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.7706\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 0.7517\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 10s 55ms/step - loss: 0.7358\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.7206\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.7093\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 9s 55ms/step - loss: 0.6966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f010893c898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR2Uaer5oarj"
      },
      "source": [
        "**Generating the Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ0KcH2FrFHj",
        "outputId": "6d898012-801a-4305-886b-919ac6e7460c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/checkpoints/ckpt_30'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfAtUY1vrHYz"
      },
      "source": [
        "model = build_model(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NdgU-8QrTTt",
        "outputId": "e3a300d8-5afb-4213-ef29-274665bcfe57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4Zu-WRDnWjw"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9VyuZboodyj",
        "outputId": "3d49138f-6701-4ead-edb9-d6aa71d166c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: No matter, here's the poor worm,\n",
            "For one to tear faith, 'tis a good lord Gedest,\n",
            "And with the lloke revenges awhile\n",
            "Diving Edward's fault when women are so becoming, indeed!\n",
            "This bastard by the hollow of it up on high.\n",
            "\n",
            "HENRY PERCY:\n",
            "Because your lordship morning excuse what they\n",
            "ere I want me: though his father died,\n",
            "Whose ensue of his substance of my head\n",
            "And threat the glory of my part shall be awhile:\n",
            "Digst put on my knighthood and therein values this?\n",
            "\n",
            "POLIXENES:\n",
            "I will, I am\n",
            "to demand and girl in that dear people!\n",
            "Come you to angry bitterly of your brother\n",
            "Does for both my madren traitors and piecely since,\n",
            "My brother Montague shall you to church.\n",
            "For shame, my lord, here comes!\n",
            "\n",
            "LUCENTIO:\n",
            "Brother, how nged in the world.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "Rare I not proverbroom thee,\n",
            "That any buy to hear me, sweet sir; what says Romeo, will ke now\n",
            "Thy play she is a worthy fear; ye thou wilt stay with her!\n",
            "\n",
            "SLY:\n",
            "Are you a store of crowns.\n",
            "\n",
            "PAULINA:\n",
            "From what you please.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "By heaven, I\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyaOHjYzupYi"
      },
      "source": [
        ""
      ],
      "execution_count": 57,
      "outputs": []
    }
  ]
}