{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practicing Text Generation one more time.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMeTY/nFzYZEJRyxIFmonTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumanyurosha/tensorflow-specialization/blob/master/Practice/Practicing_Text_Generation_one_more_time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOIjYBM3xiUK"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow import keras"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSM5uToZ0rJk"
      },
      "source": [
        "# **Downloading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIfWwNsTynZz",
        "outputId": "8f701a3e-4a5a-4690-c690-24645f23d51e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "shakespeare_url = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare\", shakespeare_url)\n",
        "\n",
        "with open(filepath, \"r\") as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "print(\"The size of Dataset is :{}\".format(len(text_data)))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of Dataset is :1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxYQ_sov0vD_"
      },
      "source": [
        "# **Analyzing the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCQV4tBw0IJu",
        "outputId": "4bf7cdd0-1a16-488f-c285-7501dc496d30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text_data[:250])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOnEUOKXzZJq",
        "outputId": "9666f68b-ac5e-41fa-88ec-8b2219e31d1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = sorted(set(text_data))\n",
        "print(\"Unique characters in the Dataset are : {}\".format(len(vocab)))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters in the Dataset are : 65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g91uNYaSzoZ1",
        "outputId": "504f320a-1eee-4c87-9a1e-6dc33ec61a73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for char, i in zip(vocab, range(20)):\n",
        "    print(\"{} : {}\".format(i+1, repr(char)))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 : '\\n'\n",
            "2 : ' '\n",
            "3 : '!'\n",
            "4 : '$'\n",
            "5 : '&'\n",
            "6 : \"'\"\n",
            "7 : ','\n",
            "8 : '-'\n",
            "9 : '.'\n",
            "10 : '3'\n",
            "11 : ':'\n",
            "12 : ';'\n",
            "13 : '?'\n",
            "14 : 'A'\n",
            "15 : 'B'\n",
            "16 : 'C'\n",
            "17 : 'D'\n",
            "18 : 'E'\n",
            "19 : 'F'\n",
            "20 : 'G'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQYpjtW403aH"
      },
      "source": [
        "# **Creating a Vocabulary out of the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfEyE2Vz1S4",
        "outputId": "5a710688-0b16-457a-94f7-3de9b9e25f44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "char2idx = {char:i for i, char in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# converting text into a sequence of integers\n",
        "text_as_int = np.array([char2idx[char] for char in text_data])\n",
        "\n",
        "print(repr(text_data[:13]))\n",
        "print(text_as_int[:13])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen'\n",
            "[18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTJw7_Zv2NfL"
      },
      "source": [
        "# **Setting Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDi_oB152M12"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "batch_size = 32\n",
        "seq_len = 100\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL9HEXXi2glj"
      },
      "source": [
        "# **Creating a Dataset for our Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubjCCvyF22ZJ",
        "outputId": "3d2c8728-9bb9-49a4-d898-6041100ab73f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# creating a Dataset with batches equal to sequence length + 1 (+ 1 for target)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "dataset = dataset.batch(seq_len + 1, drop_remainder=True)\n",
        "\n",
        "# shape should be (101, )\n",
        "dataset"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: (101,), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5d0ghBV383Z"
      },
      "source": [
        "# **Creating Input and Target Values for our Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgmUGfKQ2gCH"
      },
      "source": [
        "def seperate_input_target(chunk):\n",
        "    # all except the last\n",
        "    input_text = chunk[:-1]\n",
        "    # all except the first\n",
        "    output_text = chunk[1:]\n",
        "\n",
        "    return input_text, output_text\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7672eiH1lcG",
        "outputId": "c1194e72-50b4-41b6-f343-c596f3085835",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset = dataset.map(seperate_input_target)\n",
        "\n",
        "# shape should be (100, ), (100, )\n",
        "dataset"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: ((100,), (100,)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHSi_9sr6OJM"
      },
      "source": [
        "# **Checking what our model will see**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXITpL4v6Jj1",
        "outputId": "84e800fb-beff-4c03-b19f-289fb43fe19b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input, output in dataset.take(1):\n",
        "    print(\"Input : {}\".format(repr(\"\".join([idx2char[char] for char in input]))))\n",
        "    print()\n",
        "    print(\"Output: {}\".format(repr(\"\".join([idx2char[char] for char in output]))))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "\n",
            "Output: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwYjEa657Sg5"
      },
      "source": [
        "# **Creating batches of our Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZHZJBIB5rWq",
        "outputId": "d5eff6ab-458a-4f44-8ca2-bb2bf70fbddd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "# shape should be (32, 100), (32, 100)\n",
        "dataset"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((32, 100), (32, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cO1Lp0q4l91"
      },
      "source": [
        "# **Creating a Model for our Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hYejVwn4H6T"
      },
      "source": [
        "def build_model(batch_size, rnn_units, vocab_size, embedding_dim):\n",
        "\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                               batch_input_shape=[batch_size,None]),\n",
        "        keras.layers.GRU(128, dropout=0.2, stateful=True, return_sequences=True),\n",
        "        keras.layers.GRU(128, dropout=0.2, stateful=True, return_sequences=True),\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(vocab_size,\n",
        "         activation=\"softmax\"))\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW6KDsZJRP0w"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return keras.losses.sparse_categorical_crossentropy( labels, logits,\n",
        "                                                        from_logits=True,)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGYYprrr5psr",
        "outputId": "df639cc9-ac13-4263-f855-bfc1a1d12168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = build_model(batch_size=batch_size, \n",
        "                    rnn_units=rnn_units,\n",
        "                    vocab_size=vocab_size,\n",
        "                    embedding_dim=embedding_dim)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"adam\")"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (32, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru_8 (GRU)                  (32, None, 128)           148224    \n",
            "_________________________________________________________________\n",
            "gru_9 (GRU)                  (32, None, 128)           99072     \n",
            "_________________________________________________________________\n",
            "time_distributed_7 (TimeDist (None, None, 65)          8385      \n",
            "=================================================================\n",
            "Total params: 272,321\n",
            "Trainable params: 272,321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2QjRQk_8o2p"
      },
      "source": [
        "# **Checking the Model beahviour and Output shape**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmL0R7vb9rpb",
        "outputId": "218da4a0-ccb1-40c0-cdff-7ff33add2563",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input, output in dataset.take(1):  \n",
        "    predicted_example = model(input)\n",
        "    print(predicted_example.shape)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 100, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2jD7uXnGQgI",
        "outputId": "0cdef6a2-cca7-4f64-9e19-bd0c6c444d9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predicted_example[0][0]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(65,), dtype=float32, numpy=\n",
              "array([0.01535656, 0.01547176, 0.01537743, 0.01522947, 0.01519165,\n",
              "       0.01546737, 0.01548958, 0.01542859, 0.01532648, 0.01538258,\n",
              "       0.01534014, 0.01533251, 0.01539102, 0.01533142, 0.01520745,\n",
              "       0.01551652, 0.0154529 , 0.01543395, 0.01548969, 0.01544755,\n",
              "       0.01539544, 0.01545648, 0.01540778, 0.01536581, 0.01552243,\n",
              "       0.01527205, 0.01532785, 0.01541418, 0.01546018, 0.01531322,\n",
              "       0.01538095, 0.01531458, 0.01531058, 0.01557555, 0.01537086,\n",
              "       0.01536181, 0.01537607, 0.01539904, 0.01542821, 0.01540883,\n",
              "       0.01540503, 0.01524932, 0.01538644, 0.01539191, 0.01533041,\n",
              "       0.01555246, 0.01540336, 0.01532918, 0.01537277, 0.0153609 ,\n",
              "       0.01554938, 0.01541056, 0.01542772, 0.01538872, 0.01543457,\n",
              "       0.01547509, 0.0152868 , 0.01527127, 0.0153402 , 0.01533103,\n",
              "       0.01529409, 0.01530994, 0.01555583, 0.01530431, 0.01531223],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1djDWGDF82St",
        "outputId": "0971b449-ee65-48ec-b490-90839816bc47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# select a sequence from the batch\n",
        "# here predicted_example[0] has a shape (seq_len, vocab_size)\n",
        "# where row[i, :] contains prob for each word in the vocab for the ith word in the sequence\n",
        "prediction = tf.random.categorical(predicted_example[0], num_samples=1)\n",
        "prediction = np.argmax(predicted_example[0], axis=1)\n",
        "#prediction = tf.squeeze(prediction, axis=-1)\n",
        "prediction"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([33, 18, 18, 18, 18, 24, 24, 24, 24, 24, 24, 36, 37, 37, 37, 37, 55,\n",
              "       55, 55, 47, 36, 64, 28, 18, 21, 18, 18, 61, 61, 62, 62, 31, 64, 61,\n",
              "       61, 64, 24, 24, 24, 24,  1, 26, 52, 18, 64, 64, 21, 13, 18, 18, 21,\n",
              "       64, 18, 21, 21, 30, 30, 33, 18, 18, 22, 64, 13, 18, 61, 61, 18,  2,\n",
              "        2,  2, 53, 37, 23, 24, 24, 24, 24, 24, 43, 61, 18, 18, 21, 19, 18,\n",
              "       18, 60, 18, 18, 24, 24, 24, 37, 37, 62, 30, 62, 18, 24, 31])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PESSMD-AWHc5",
        "outputId": "f5ec2d0f-adfa-4e3d-d9e9-5ca0d16d03d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prediction.shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VHQniLbCIAr",
        "outputId": "31fac048-7257-4368-cd55-10b769755154",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we should expect very random sequence because the model is not trained yet\n",
        "print(repr(\"\".join([idx2char[c] for c in prediction])))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'UFFFFLLLLLLXYYYYqqqiXzPFIFFwwxxSzwwzLLLL NnFzzIAFFIzFIIRRUFFJzAFwwF!!!oYKLLLLLewFFIGFFvFFLLLYYxRxFLS'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wH7SIjdKZo_"
      },
      "source": [
        "# **Creating a Checkpoint Callback for saving our Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRAuA2gUCtQ-"
      },
      "source": [
        "checkpoint_dir = \"/checkpoints\"\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"{epoch}\")\n",
        "\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(checkpoint_prefix,\n",
        "                                                      save_weights_only=True)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7voTS07yK5yi"
      },
      "source": [
        "# **Training our Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywO8lvAfK4UM",
        "outputId": "761c2a44-64c0-4d1b-a068-676cd406923a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=30, callbacks=[checkpoint_callback], verbose=1)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.layer.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.layer.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.layer.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 2.5074\n",
            "Epoch 2/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.9505\n",
            "Epoch 3/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.7941\n",
            "Epoch 4/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.7103\n",
            "Epoch 5/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.6590\n",
            "Epoch 6/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.6243\n",
            "Epoch 7/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.5996\n",
            "Epoch 8/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.5776\n",
            "Epoch 9/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.5625\n",
            "Epoch 10/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.5490\n",
            "Epoch 11/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.5379\n",
            "Epoch 12/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.5272\n",
            "Epoch 13/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.5203\n",
            "Epoch 14/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.5129\n",
            "Epoch 15/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.5066\n",
            "Epoch 16/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.5005\n",
            "Epoch 17/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4953\n",
            "Epoch 18/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4902\n",
            "Epoch 19/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4860\n",
            "Epoch 20/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4828\n",
            "Epoch 21/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4788\n",
            "Epoch 22/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4753\n",
            "Epoch 23/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4721\n",
            "Epoch 24/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4698\n",
            "Epoch 25/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4664\n",
            "Epoch 26/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4643\n",
            "Epoch 27/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4610\n",
            "Epoch 28/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4584\n",
            "Epoch 29/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4573\n",
            "Epoch 30/30\n",
            "345/345 [==============================] - 4s 12ms/step - loss: 1.4551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nI5BrcKLlwb"
      },
      "source": [
        "# **Building a new model with saved weights but different batch size for Text Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq7QAgRBMMSG",
        "outputId": "7f7e9708-5fec-4295-b125-5fca51844e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/checkpoints/30'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNWcOKg4LlMB",
        "outputId": "bc2ee842-cd15-4d34-b47a-f71c9f3f1557",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = build_model(batch_size=1, vocab_size=vocab_size,\n",
        "                    embedding_dim=embedding_dim, rnn_units=rnn_units)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "# here the batch_size should be 1\n",
        "model.summary()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_10 (GRU)                 (1, None, 128)            148224    \n",
            "_________________________________________________________________\n",
            "gru_11 (GRU)                 (1, None, 128)            99072     \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, None, 65)          8385      \n",
            "=================================================================\n",
            "Total params: 272,321\n",
            "Trainable params: 272,321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQzmxUcrLFbS"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "    text_length = 1000\n",
        "    text_generated = []\n",
        "\n",
        "    input_sequence = np.array([char2idx[c] for c in start_string])\n",
        "    input_sequence = tf.expand_dims(input_sequence, 0)\n",
        "\n",
        "    model.reset_states()\n",
        "    for i in range(text_length):\n",
        "        prediction = model(input_sequence)\n",
        "        prediction = tf.squeeze(prediction, 0)\n",
        "        prediction = tf.math.log(prediction)\n",
        "        prediction_id = tf.random.categorical(prediction, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_sequence = tf.expand_dims([prediction_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[prediction_id])\n",
        "        \n",
        "\n",
        "    return start_string + \"\".join(text_generated)\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPi-1BnBPSuy",
        "outputId": "ed2ed4e2-75f7-4385-9db4-04df820495fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(generate_text(model, \"Romeo:\"))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Romeo:\n",
            "In heaven death, as in the buny 'eg, my lords,\n",
            "The first butney, let me this glory on my not:\n",
            "List and ruilth sad day:\n",
            "All Henry that I have indeed this your peing?\n",
            "\n",
            "Lord:\n",
            "Petake us never scorn;\n",
            "And still well, such Lancaster, methink bean all widowr.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Say, nor haughs and him.\n",
            "\n",
            "POMPEY:\n",
            "Ey, nor much, I'll had fales:\n",
            "Two woman is so Of segners.\n",
            "\n",
            "FLORIZEL:\n",
            "Sweet, good quarred their son: I were immether.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Come, that were as doom,---\n",
            "\n",
            "KING HERRY VI:\n",
            "Sir? A wild chambors of the malsh!\n",
            "His loss te in the wedd-missed to the sire\n",
            "Than eye a Montague;\n",
            "I please--Boldal, if once was need becauson'-thought,\n",
            "And you or though leave a licken than no grace, what of my hasom: so fellow?\n",
            "\n",
            "MERCUTIO:\n",
            "Imaballal life adverting words of the soulf his court:\n",
            "He will make it but to your talk you, day!\n",
            "\n",
            "THOMAMERLE:\n",
            "When I beseech you are a reasons; I think not on an't so as you sworn: in the head, what courvers\n",
            "Is be\n",
            "I never leands all under-boy Down:\n",
            "I would not ralner with hea\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUFujPT7Ph0b"
      },
      "source": [
        ""
      ],
      "execution_count": 95,
      "outputs": []
    }
  ]
}