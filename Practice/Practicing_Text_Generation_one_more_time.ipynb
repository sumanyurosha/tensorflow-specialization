{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practicing Text Generation one more time.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfdUa2RPbuhTqBv8EHsOmI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumanyurosha/tensorflow-specialization/blob/master/Practice/Practicing_Text_Generation_one_more_time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOIjYBM3xiUK"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSM5uToZ0rJk"
      },
      "source": [
        "# **Downloading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIfWwNsTynZz",
        "outputId": "6bede185-0826-4da9-8e64-a17b3114f7a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "shakespeare_url = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare\", shakespeare_url)\n",
        "\n",
        "with open(filepath, \"r\") as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "print(\"The size of Dataset is :{}\".format(len(text_data)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size of Dataset is :1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxYQ_sov0vD_"
      },
      "source": [
        "# **Analyzing the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCQV4tBw0IJu",
        "outputId": "a63df8f3-848e-4677-93d1-ee3913ce4c95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text_data[:250])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOnEUOKXzZJq",
        "outputId": "f99819a5-d3eb-43d7-ecd8-a0586d66514e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = sorted(set(text_data))\n",
        "print(\"Unique characters in the Dataset are : {}\".format(len(vocab)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters in the Dataset are : 65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g91uNYaSzoZ1",
        "outputId": "12e36d86-eec6-47e1-d60d-6dc1847cb465",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for char, i in zip(vocab, range(20)):\n",
        "    print(\"{} : {}\".format(i+1, repr(char)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 : '\\n'\n",
            "2 : ' '\n",
            "3 : '!'\n",
            "4 : '$'\n",
            "5 : '&'\n",
            "6 : \"'\"\n",
            "7 : ','\n",
            "8 : '-'\n",
            "9 : '.'\n",
            "10 : '3'\n",
            "11 : ':'\n",
            "12 : ';'\n",
            "13 : '?'\n",
            "14 : 'A'\n",
            "15 : 'B'\n",
            "16 : 'C'\n",
            "17 : 'D'\n",
            "18 : 'E'\n",
            "19 : 'F'\n",
            "20 : 'G'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQYpjtW403aH"
      },
      "source": [
        "# **Creating a Vocabulary out of the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBfEyE2Vz1S4",
        "outputId": "26858319-bc36-45a8-f9c3-379cd12e9b05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "char2idx = {char:i for i, char in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# converting text into a sequence of integers\n",
        "text_as_int = np.array([char2idx[char] for char in text_data])\n",
        "\n",
        "print(repr(text_data[:13]))\n",
        "print(text_as_int[:13])"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen'\n",
            "[18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTJw7_Zv2NfL"
      },
      "source": [
        "# **Setting Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDi_oB152M12"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "batch_size = 32\n",
        "seq_len = 100\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL9HEXXi2glj"
      },
      "source": [
        "# **Creating a Dataset for our Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubjCCvyF22ZJ",
        "outputId": "cdef6e99-cfdb-49f6-9e16-e957cfe19438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# creating a Dataset with batches equal to sequence length + 1 (+ 1 for target)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "dataset = dataset.batch(seq_len + 1, drop_remainder=True)\n",
        "\n",
        "# shape should be (101, )\n",
        "dataset"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: (101,), types: tf.int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5d0ghBV383Z"
      },
      "source": [
        "# **Creating Input and Target Values for our Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgmUGfKQ2gCH"
      },
      "source": [
        "def seperate_input_target(chunk):\n",
        "    # all except the last\n",
        "    input_text = chunk[:-1]\n",
        "    # all except the first\n",
        "    output_text = chunk[1:]\n",
        "\n",
        "    return input_text, output_text\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7672eiH1lcG",
        "outputId": "d5230f60-355e-48c9-c8cb-51ca7aabefdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset = dataset.map(seperate_input_target)\n",
        "\n",
        "# shape should be (100, ), (100, )\n",
        "dataset"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: ((100,), (100,)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHSi_9sr6OJM"
      },
      "source": [
        "# **Checking what our model will see**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXITpL4v6Jj1",
        "outputId": "65c4a4b1-6cfa-4969-f7bf-e6ec54ff44df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input, output in dataset.take(1):\n",
        "    print(\"Input : {}\".format(repr(\"\".join([idx2char[char] for char in input]))))\n",
        "    print()\n",
        "    print(\"Output: {}\".format(repr(\"\".join([idx2char[char] for char in output]))))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "\n",
            "Output: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwYjEa657Sg5"
      },
      "source": [
        "# **Creating batches of our Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZHZJBIB5rWq",
        "outputId": "80459609-d70f-48f4-dca8-e1990b71e0f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset = dataset.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "# shape should be (32, 100), (32, 100)\n",
        "dataset"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((32, 100), (32, 100)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cO1Lp0q4l91"
      },
      "source": [
        "# **Creating a Model for our Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hYejVwn4H6T"
      },
      "source": [
        "def build_model(batch_size, rnn_units, vocab_size, embedding_dim):\n",
        "\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                               batch_input_shape=[batch_size,None]),\n",
        "        keras.layers.LSTM(rnn_units, return_sequences=True),\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(vocab_size,\n",
        "         activation=\"softmax\"))\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW6KDsZJRP0w"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return keras.losses.sparse_categorical_crossentropy( labels, logits,\n",
        "                                                        from_logits=True,)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGYYprrr5psr",
        "outputId": "4c6f99b2-0407-4c34-d8ff-0cfe2247812d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = build_model(batch_size=batch_size, \n",
        "                    rnn_units=rnn_units,\n",
        "                    vocab_size=vocab_size,\n",
        "                    embedding_dim=embedding_dim)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"adam\")"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (32, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (32, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, None, 65)          66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2QjRQk_8o2p"
      },
      "source": [
        "# **Checking the Model beahviour and Output shape**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmL0R7vb9rpb",
        "outputId": "bbbbb2ec-90b3-438f-b4ee-0692fb9000dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for input, output in dataset.take(1):  \n",
        "    predicted_example = model(input)\n",
        "    print(predicted_example.shape)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 100, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2jD7uXnGQgI",
        "outputId": "1d20f90c-4f29-4069-ca20-22a408ab0b9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predicted_example[0][0]"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(65,), dtype=float32, numpy=\n",
              "array([1.8489206e-02, 2.9586709e-01, 1.9500013e-02, 8.3192060e-07,\n",
              "       1.0284434e-06, 8.0131441e-03, 4.8147071e-02, 4.2435517e-05,\n",
              "       1.2373032e-02, 7.1150034e-06, 1.8079085e-02, 1.0799175e-02,\n",
              "       6.5165604e-03, 8.0820973e-07, 3.0882195e-08, 6.2296683e-09,\n",
              "       2.8834386e-08, 7.0053567e-08, 4.8825888e-07, 1.5504098e-08,\n",
              "       1.7830434e-09, 5.8081209e-06, 7.9266854e-08, 1.4234587e-07,\n",
              "       1.4980741e-06, 5.6981753e-07, 9.1497044e-05, 6.6821769e-07,\n",
              "       7.5054082e-08, 3.7964909e-07, 9.3837247e-07, 9.9987255e-06,\n",
              "       2.2691697e-06, 1.1426713e-07, 3.6605554e-06, 7.4559679e-08,\n",
              "       1.7281325e-05, 1.7665116e-08, 3.0462110e-07, 4.7049444e-02,\n",
              "       1.8218769e-05, 9.1463597e-03, 3.0167127e-02, 4.7915582e-02,\n",
              "       5.7611475e-03, 4.7673788e-03, 1.6211727e-07, 7.8390073e-03,\n",
              "       1.5253588e-06, 7.7585474e-04, 2.5885260e-02, 9.9517563e-03,\n",
              "       6.9462299e-02, 5.2193092e-05, 8.1354994e-03, 1.4053859e-03,\n",
              "       1.6244106e-01, 8.1612378e-02, 2.2407152e-02, 4.9273421e-08,\n",
              "       8.4463097e-03, 5.6744223e-03, 2.6931567e-03, 1.0415028e-02,\n",
              "       3.6144634e-06], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1djDWGDF82St",
        "outputId": "456567e3-cd00-40a5-ed1b-3889ed3dd8fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# select a sequence from the batch\n",
        "# here predicted_example[0] has a shape (seq_len, vocab_size)\n",
        "# where row[i, :] contains prob for each word in the vocab for the ith word in the sequence\n",
        "prediction = tf.random.categorical(predicted_example[0], num_samples=1)\n",
        "prediction = np.argmax(predicted_example[0], axis=1)\n",
        "#prediction = tf.squeeze(prediction, axis=-1)\n",
        "prediction"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  1, 21, 53, 40, 43,  1, 47,  5,  1, 58, 46, 43,  1, 61, 53, 56,\n",
              "       50, 42,  6,  0, 31, 53,  1, 57, 47, 53, 52,  1, 39, 57,  1, 63, 53,\n",
              "       59, 56, 57,  1, 41, 53, 52, 50, 42,  1, 44, 47, 52,  1, 51, 43, 10,\n",
              "        1, 57, 53,  1, 47, 58,  1, 57, 46, 53, 59, 50, 42,  1, 52, 53, 58,\n",
              "        6,  0, 35, 43, 56, 43,  1, 58, 46, 43, 56, 43, 47, 52, 43, 61, 43,\n",
              "       57, 57, 47, 58, 63,  1, 47, 52,  1, 63, 53, 59, 56,  1, 45])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PESSMD-AWHc5",
        "outputId": "4084e614-f885-4819-8e70-1bb0560c37ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prediction.shape"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VHQniLbCIAr",
        "outputId": "a76d2956-8d34-472c-88ee-7c942482cd19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we should expect very random sequence because the model is not trained yet\n",
        "print(repr(\"\".join([idx2char[c] for c in prediction])))"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"  Iobe i' the world,\\nSo sion as yours conld fin me: so it should not,\\nWere thereinewessity in your g\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wH7SIjdKZo_"
      },
      "source": [
        "# **Creating a Checkpoint Callback for saving our Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRAuA2gUCtQ-"
      },
      "source": [
        "checkpoint_dir = \"/checkpoints\"\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"{epoch}\")\n",
        "\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(checkpoint_prefix,\n",
        "                                                      save_weights_only=True)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7voTS07yK5yi"
      },
      "source": [
        "# **Training our Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywO8lvAfK4UM",
        "outputId": "691197f8-8bf1-423c-be1a-d05e1057c53e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=30, callbacks=[checkpoint_callback], verbose=1)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "345/345 [==============================] - 14s 40ms/step - loss: 2.5082\n",
            "Epoch 2/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.8195\n",
            "Epoch 3/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.5752\n",
            "Epoch 4/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.4503\n",
            "Epoch 5/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.3745\n",
            "Epoch 6/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.3195\n",
            "Epoch 7/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.2751\n",
            "Epoch 8/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.2345\n",
            "Epoch 9/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.1955\n",
            "Epoch 10/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.1575\n",
            "Epoch 11/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.1171\n",
            "Epoch 12/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.0736\n",
            "Epoch 13/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 1.0276\n",
            "Epoch 14/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.9784\n",
            "Epoch 15/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.9265\n",
            "Epoch 16/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.8728\n",
            "Epoch 17/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.8175\n",
            "Epoch 18/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.7638\n",
            "Epoch 19/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.7117\n",
            "Epoch 20/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.6611\n",
            "Epoch 21/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.6170\n",
            "Epoch 22/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.5721\n",
            "Epoch 23/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.5344\n",
            "Epoch 24/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.5024\n",
            "Epoch 25/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.4719\n",
            "Epoch 26/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.4427\n",
            "Epoch 27/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.4200\n",
            "Epoch 28/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.3993\n",
            "Epoch 29/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.3788\n",
            "Epoch 30/30\n",
            "345/345 [==============================] - 14s 41ms/step - loss: 0.3714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nI5BrcKLlwb"
      },
      "source": [
        "# **Building a new model with saved weights but different batch size for Text Generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq7QAgRBMMSG",
        "outputId": "055334a2-01e9-4d4b-ec21-5e90c50454be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/checkpoints/30'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNWcOKg4LlMB",
        "outputId": "f5c691db-6bef-4f9b-815c-aa01d3ac1149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = build_model(batch_size=1, vocab_size=vocab_size,\n",
        "                    embedding_dim=embedding_dim, rnn_units=rnn_units)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "# here the batch_size should be 1\n",
        "model.summary()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "lstm_17 (LSTM)               (1, None, 1024)           5246976   \n",
            "_________________________________________________________________\n",
            "time_distributed_9 (TimeDist (None, None, 65)          66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQzmxUcrLFbS"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "\n",
        "    text_length = 1000\n",
        "    text_generated = []\n",
        "\n",
        "    input_sequence = np.array([char2idx[c] for c in start_string])\n",
        "    input_sequence = tf.expand_dims(input_sequence, 0)\n",
        "\n",
        "    for i in range(text_length):\n",
        "        prediction = model(input_sequence)\n",
        "        # prediction = tf.squeeze(prediction, 0)\n",
        "        # prediction_id = tf.random.categorical(prediction, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        prediction = np.argmax(prediction, axis=-1)\n",
        "        prediction_id = prediction[-1, 0]\n",
        "        input_sequence = tf.expand_dims([prediction_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[prediction_id])\n",
        "        \n",
        "\n",
        "    return start_string + \"\".join(text_generated)\n"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPi-1BnBPSuy",
        "outputId": "9332872d-bb3e-429d-e141-66ad4e387610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(generate_text(model, \"Romeo:\"))"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Romeo:I the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUFujPT7Ph0b"
      },
      "source": [
        ""
      ],
      "execution_count": 159,
      "outputs": []
    }
  ]
}